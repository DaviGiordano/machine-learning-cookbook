{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimers:**\n",
    "- All code was typed by me, not copied and pasted\n",
    "- The code is from the book \"Machine Learning with python cookbook\" by Chris Albon (1 ed). \n",
    "- All comments are written in my own words, based on my previous knowledge and on the comments of the book. \n",
    "- My comments are usually short and concise, since the goal of this notebook is to implement the code, not to teach about the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18. Naive Bayes\n",
    "## 18.0 Introduction\n",
    "\n",
    "- The Naive Bayes classifier will predict the class with the highest probability _a posteriori_.\n",
    "- Assuming that the features of a dataset are independent, the full likelihood can be computed as the product of the likelihood for each feature (_Naive_ Assumption)\n",
    "- We have to previously define the statistical distribution of the likelihood (usually Gaussian, Multinomial or Bernoulli).\n",
    "- To classify a new observation, compute the probability a posteriori of each class with the likelihood and the _prior_ probability of each class. The clas with the highest posterior probability is the prediction of the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.1 Training a Classifier for Continuous Features\n",
    "**Problem**\n",
    "- Train a Naive Bayes Classifier with a dataset that has continuous features only\n",
    "**Solution:**\n",
    "- Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "classifier = GaussianNB()\n",
    "\n",
    "model = classifier.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_observation = [[4, 4, 4, 0.4]]\n",
    "\n",
    "model.predict(new_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "- In the previous case, the priors for each class were adjusted with the dataset (frequentist approach). \n",
    "- We can define prior probabilities for each class, as bellow:\n",
    "- NOTE: This is approach could be more _bayesian_ if we did some Bayesian Modelling hehe. (See the pymc libary and this pydata talk, as an introduction)\n",
    "- https://www.youtube.com/watch?v=911d4A1U0BE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GaussianNB(priors=[0.25, 0.25, 0.5])\n",
    "model = clf.fit(features, target)\n",
    "model.predict(new_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "- The posterior probabilities are not useful! They should not be interpreted as the probability for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.2 Training a Classifier for Discrete and Count Features\n",
    "**Problem:**\n",
    "- Given discrete or count data, train a Naive Bayes Classifier\n",
    "\n",
    "**Solution:**\n",
    "- The dreaded but amazing Multinomial distribution\n",
    "- See this video for an intro to the Multinomial: \n",
    "- https://www.youtube.com/watch?v=Dkc_hcVWDpA\n",
    "- Yes, I do feel very _statistical_ by knowing a bit about the Multinomial Distribution xD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    "                      'Brazil is best',\n",
    "                      'Germany beats both'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:**\n",
    "- Hahahahah this is actually the example from the book. See page 333.\n",
    "- I'm pretty sure he's referencing to the World Cup and the 7x1 \n",
    "- It is crazy to find a reference of this in a ML book.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 2, 0, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 1, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create bag of words - Bilbo would be proud\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "\n",
    "features = bag_of_words.toarray()\n",
    "target = np.array([0, 0, 1])\n",
    "\n",
    "# Note that Brazil is indice 4\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MultinomialNB(class_prior=[0.25, 0.5])\n",
    "model = classifier.fit(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "- We are predicting if a message (tweet?) comes from a pro-brazil or pro-germany person.\n",
    "- We created a bag of words to encode the text data. We could have better encoded it\n",
    "- We specified the prior probabilities of each class\n",
    "- MultinomialNB has an ``alpha`` parameter that does _smoothing_. *No idea what this is. Research later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.3 Training a Naive Bayes Classifier for Binary Features\n",
    "**Problem:**\n",
    "- Given binary data, train a Naive Bayes Classifier\n",
    "\n",
    "**Solution:**\n",
    "- The Bernoulli distribution, mother of all distribs\n",
    "- All hail the Bernoulli!\n",
    "- Lots of jokes in this notebook, huh? Sorry for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "features = np.random.randint(2, size=(100, 3))\n",
    "target = np.random.randint(2, size=(100,1)).ravel() # Why use ravel here and not just specify size=100?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = BernoulliNB(class_prior=[0.25, 0.5])\n",
    "model = classifier.fit(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "- Bernoulli also has an $\\alpha$ smoothing parameter\n",
    "- In all NB cases, we can specify an uniform prior by setting `fit_prior=False`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.4 Training a Naive Bayes Classifier for Binary Features\n",
    "**Problem:**\n",
    "- Calibrating the predicted probabilities from the NB classifiers so they are interpretable! (They are the probability of that class)\n",
    "\n",
    "**Solution:**\n",
    "- ``CalibratedClassifierCV``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31859969, 0.63663466, 0.04476565]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "classifier = GaussianNB()\n",
    "\n",
    "classifier_sigmoid = CalibratedClassifierCV(classifier, cv=2, method='sigmoid')\n",
    "\n",
    "classifier_sigmoid.fit(features, target)\n",
    "\n",
    "new_observation = [[2.6, 2.6, 2.6, 0.4]]\n",
    "\n",
    "classifier_sigmoid.predict_proba(new_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "- We are calibrating the posterior probabilities of each class using a sigmoid function\n",
    "- I understand that we are applying the sigmoid function to turn the results into probabilities (\"squashing\" it between 0 and 1).\n",
    "- We are doing this k=2 times (k-fold cross validation), and taking the average result. \n",
    "- What I don't understand is why exactly we are doing cross validation here...\n",
    "- In the book, it mentions that there are two possible calibration methods.\n",
    "- Research this later!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tecnico_sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
